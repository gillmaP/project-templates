# Hybrid Template (Accelerate + Hydra)

A hybrid template combining Accelerate's automatic distributed training with Hydra's powerful configuration management.

## ğŸ“‹ Structure

```
project/
â”œâ”€â”€ main.py                 # Hydra + Accelerate
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ data/
â”‚   â””â”€â”€ train/
â”œâ”€â”€ train/
â”‚   â””â”€â”€ trainer.py          # Uses Accelerate
â”œâ”€â”€ models/
â””â”€â”€ utils/
```

## ğŸ¯ Features

- **Hydra**: Powerful configuration management, experiment tracking
- **Accelerate**: Automatic distributed training, code simplicity
- **Best of Both Worlds**: Combines advantages of both approaches

## ğŸš€ Usage

### Single GPU
```bash
python main.py
```

### Multi GPU
```bash
accelerate launch main.py
# or
torchrun --nproc_per_node=4 main.py
```

### Configuration Override
```bash
python main.py train.learning_rate=1e-5
```

## ğŸ“ Key Code Patterns

### Hydra + Accelerate Combination
```python
from accelerate import Accelerator
import hydra
from omegaconf import DictConfig

@hydra.main(config_path="configs", config_name="config")
def main(cfg: DictConfig):
    # Configuration management with Hydra
    accelerator = Accelerator(
        mixed_precision=cfg.train.mixed_precision,
        log_with="tensorboard",
        project_dir=cfg.experiment.log_dir
    )
    # Automatic distributed training with Accelerate
    model, opt = accelerator.prepare(model, optimizer)
```

## âœ… Pros

- Hydra's configuration management + Accelerate's automation
- Experiment tracking + code simplicity
- Hyperparameter sweeps + automatic multi-GPU support

## ğŸ“Š Comparison

| Feature | Accelerate Only | Hydra Only | Hybrid |
|---------|----------------|------------|--------|
| Config Management | Basic | Powerful | Powerful |
| Distributed Training | Automatic | Manual | Automatic |
| Experiment Tracking | Manual | Automatic | Automatic |
| Code Simplicity | High | Medium | High |
